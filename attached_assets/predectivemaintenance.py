# -*- coding: utf-8 -*-
"""Predectivemaintenance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VFaCGw2__V08Fg70H_ZOvEVdWSr0jiFW
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, StratifiedKFold

df = pd.read_csv('/content/ai4i2020.csv')

print(df.head())
print(df.info())
print(df.describe())

df.drop(['UDI', 'Product ID'], axis=1, inplace=True)

print(df.isnull().sum())

# 6. One-Hot Encode 'Type' column
df = pd.get_dummies(df, columns=['Type'], drop_first=True)

# 7. Feature Scaling
features = df.drop('Machine failure', axis=1)
target = df['Machine failure']

scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# 8. Train-test split
X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.3, stratify=target, random_state=42)

# Define stratified k-fold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Logistic Regression
lr_model = GridSearchCV(LogisticRegression(), param_grid={'C': [0.1, 1, 10]}, cv=skf, scoring='f1')
lr_model.fit(X_train, y_train)

# Random Forest
rf_model = GridSearchCV(RandomForestClassifier(), param_grid={'n_estimators': [100, 200]}, cv=skf, scoring='f1')
rf_model.fit(X_train, y_train)

# XGBoost
xgb_model = GridSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='logloss'), param_grid={'n_estimators': [100, 200]}, cv=skf, scoring='f1')
xgb_model.fit(X_train, y_train)

# SVM
svm_model = GridSearchCV(SVC(probability=True), param_grid={'C': [0.1, 1], 'kernel': ['linear', 'rbf']}, cv=skf, scoring='f1')
svm_model.fit(X_train, y_train)

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score, classification_report

# Helper function
def evaluate_model(name, model, X_test, y_test):
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    print(f"ðŸ“Š Model: {name}")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Precision:", precision_score(y_test, y_pred))
    print("Recall:", recall_score(y_test, y_pred))
    print("F1 Score:", f1_score(y_test, y_pred))
    if y_prob is not None:
        print("ROC AUC:", roc_auc_score(y_test, y_prob))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred))
    print("=" * 50)

# Evaluate all models
evaluate_model("Logistic Regression", lr_model, X_test, y_test)
evaluate_model("Random Forest", rf_model, X_test, y_test)
evaluate_model("XGBoost", xgb_model, X_test, y_test)
evaluate_model("SVM", svm_model, X_test, y_test)

from sklearn.metrics import roc_curve, auc

# Plot ROC Curves
def plot_roc(models, X_test, y_test):
    plt.figure(figsize=(10, 6))
    for name, model in models.items():
        if hasattr(model, "predict_proba"):
            y_score = model.predict_proba(X_test)[:, 1]
        else:
            continue  # SVM linear might not support `predict_proba`

        fpr, tpr, _ = roc_curve(y_test, y_score)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')

    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curves')
    plt.legend(loc='lower right')
    plt.grid()
    plt.tight_layout()
    plt.show()

models_dict = {
    "Logistic Regression": lr_model,
    "Random Forest": rf_model,
    "XGBoost": xgb_model,
    "SVM": svm_model  # optional, can skip if no predict_proba
}

plot_roc(models_dict, X_test, y_test)

# Random Forest Feature Importance
importances_rf = rf_model.best_estimator_.feature_importances_
features_list = df.drop('Machine failure', axis=1).columns

plt.figure(figsize=(10, 6))
sns.barplot(x=importances_rf, y=features_list)
plt.title('Feature Importance - Random Forest')
plt.tight_layout()
plt.show()

# XGBoost Feature Importance
importances_xgb = xgb_model.best_estimator_.feature_importances_

plt.figure(figsize=(10, 6))
sns.barplot(x=importances_xgb, y=features_list)
plt.title('Feature Importance - XGBoost')
plt.tight_layout()
plt.show()

best_model = xgb_model





from imblearn.over_sampling import SMOTE

# Assuming you already have X_train and y_train
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_res, y_res = smote.fit_resample(X_train, y_train)

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# Convert the data into DMatrix format for XGBoost (optional but recommended for performance)
dtrain = xgb.DMatrix(X_res, label=y_res)

# Specify the hyperparameters for XGBoost
params = {
    'objective': 'binary:logistic',  # For binary classification
    'eval_metric': 'logloss',  # Log loss for binary classification
    'max_depth': 6,  # You can experiment with other values
    'learning_rate': 0.1,  # You can experiment with other values
    'n_estimators': 100  # Number of trees
}

# Train the XGBoost model
model = xgb.train(params, dtrain, num_boost_round=100)

# Convert test data into DMatrix format
dtest = xgb.DMatrix(X_test)

# Make predictions
y_pred = model.predict(dtest)

# Since it's a binary classification, apply threshold (e.g., 0.5) to get class labels
y_pred_class = (y_pred > 0.5).astype(int)

# Evaluate the model
print(classification_report(y_test, y_pred_class))
print(confusion_matrix(y_test, y_pred_class))

from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [3, 6, 10],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [50, 100, 200]
}

xgb_model = xgb.XGBClassifier(objective='binary:logistic')

grid_search = GridSearchCV(xgb_model, param_grid, cv=3, n_jobs=-1)
grid_search.fit(X_res, y_res)

# Best parameters found by GridSearchCV
print(grid_search.best_params_)

# Assuming you already have the trained `model` and fitted `scaler`
sample2 = [[
    305.0,   # Air temperature [K]
    34.0,   # Process temperature [K]
    1600.0,  # Rotational speed [rpm]
    40.0,    # Torque [Nm]
    200.0,   # Tool wear [min]
    0,       # TWF
    1,       # HDF (Simulating heat dissipation issue)
    0,       # PWF
    0,       # OSF
    0,       # RNF
    1,       # Type_L
    0        # Type_M (Type = L)
]]
# Scale the input
sample_scaled = scaler.transform(sample2)

# Predict
pred = best_model.predict(sample_scaled)[0]
prob = best_model.predict_proba(sample_scaled)[0][1]

# Output
if pred == 1:
    print(f"âš  Machine failure predicted with probability {prob:.2f}")
else:
    print(f"âœ… No machine failure. Probability of failure: {prob:.2f}")

